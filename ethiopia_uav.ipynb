{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWnQMZeU5M_4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import cv2\n",
        "import albumentations as A\n",
        "\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms as T\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfeEuGri7IZw",
        "outputId": "119702e9-dca8-47c8-f262-523f8a801771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: segmentation_models_pytorch in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.7.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.6 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.24.7)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (10.4.0)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.7.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.16.0)\n",
            "Requirement already satisfied: timm==0.9.7 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.9.7)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.20.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.66.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.5.0+cu121)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.10/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation_models_pytorch) (6.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation_models_pytorch) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.26.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install segmentation_models_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHNJOVr27M5I"
      },
      "outputs": [],
      "source": [
        "import segmentation_models_pytorch as smp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUmAREHd5SAG"
      },
      "outputs": [],
      "source": [
        "class DroneDataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_path, mask_path, X, mean, std, transform=None, patch=False, img_post='.jpg', mask_post='.png'):\n",
        "        self.img_path = img_path\n",
        "        self.mask_path = mask_path\n",
        "        self.X = X\n",
        "        self.transform = transform\n",
        "        self.patches = patch\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.img_post = img_post\n",
        "        self.mask_post = mask_post\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.imread(self.img_path + '/' + self.X[idx] + self.img_post)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(self.mask_path + '/' + self.X[idx] + self.mask_post, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            aug = self.transform(image=img, mask=mask)\n",
        "            img = Image.fromarray(aug['image'])\n",
        "            mask = aug['mask']\n",
        "\n",
        "        if self.transform is None:\n",
        "            img = Image.fromarray(img)\n",
        "\n",
        "        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n",
        "        img = t(img)\n",
        "        mask = torch.from_numpy(mask).long()\n",
        "\n",
        "        if self.patches:\n",
        "            img, mask = self.tiles(img, mask)\n",
        "\n",
        "        return img, mask\n",
        "\n",
        "    def tiles(self, img, mask):\n",
        "\n",
        "        img_patches = img.unfold(1, 512, 512).unfold(2, 768, 768)\n",
        "        img_patches = img_patches.contiguous().view(3, -1, 512, 768)\n",
        "        img_patches = img_patches.permute(1, 0, 2, 3)\n",
        "\n",
        "        mask_patches = mask.unfold(0, 512, 512).unfold(1, 768, 768)\n",
        "        mask_patches = mask_patches.contiguous().view(-1, 512, 768)\n",
        "\n",
        "        return img_patches, mask_patches\n",
        "\n",
        "\n",
        "class DroneTestDataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_path, mask_path, X, transform=None, img_post='.jpg', mask_post='.png'):\n",
        "        self.img_path = img_path\n",
        "        self.mask_path = mask_path\n",
        "        self.X = X\n",
        "        self.transform = transform\n",
        "        self.img_post = img_post\n",
        "        self.mask_post = mask_post\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.imread(self.img_path + '/' + self.X[idx] + self.img_post)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(self.mask_path + '/' + self.X[idx] + self.mask_post, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            aug = self.transform(image=img, mask=mask)\n",
        "            img = Image.fromarray(aug['image'])\n",
        "            mask = aug['mask']\n",
        "\n",
        "        if self.transform is None:\n",
        "            img = Image.fromarray(img)\n",
        "\n",
        "        mask = torch.from_numpy(mask).long()\n",
        "\n",
        "        return img, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok7oZvXG5mkg"
      },
      "outputs": [],
      "source": [
        "# Metric calculation helper fucntions\n",
        "\n",
        "def pixel_accuracy(output, mask):\n",
        "    with torch.no_grad():\n",
        "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
        "        correct = torch.eq(output, mask).int()\n",
        "        accuracy = float(correct.sum()) / float(correct.numel())\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=23):\n",
        "    with torch.no_grad():\n",
        "        pred_mask = F.softmax(pred_mask, dim=1)\n",
        "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
        "        pred_mask = pred_mask.contiguous().view(-1)\n",
        "        mask = mask.contiguous().view(-1)\n",
        "\n",
        "        iou_per_class = []\n",
        "        for clas in range(0, n_classes): #loop per pixel class\n",
        "            true_class = pred_mask == clas\n",
        "            true_label = mask == clas\n",
        "\n",
        "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
        "                iou_per_class.append(np.nan)\n",
        "            else:\n",
        "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
        "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
        "\n",
        "                iou = (intersect + smooth) / (union +smooth)\n",
        "                iou_per_class.append(iou)\n",
        "        return np.nanmean(iou_per_class)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWO1Isgk5pnJ"
      },
      "outputs": [],
      "source": [
        "# Plotting helper functions\n",
        "\n",
        "def plot_loss(history):\n",
        "    plt.plot(history['val_loss'], label='val', marker='o')\n",
        "    plt.plot(history['train_loss'], label='train', marker='o')\n",
        "    plt.title('Loss per epoch');\n",
        "    plt.ylabel('loss');\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(), plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_score(history):\n",
        "    plt.plot(history['train_miou'], label='train_mIoU', marker='*')\n",
        "    plt.plot(history['val_miou'], label='val_mIoU', marker='*')\n",
        "    plt.title('Score per epoch');\n",
        "    plt.ylabel('mean IoU')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(), plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_acc(history):\n",
        "    plt.plot(history['train_acc'], label='train_accuracy', marker='*')\n",
        "    plt.plot(history['val_acc'], label='val_accuracy', marker='*')\n",
        "    plt.title('Accuracy per epoch');\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(), plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hOeNj2p51Ch"
      },
      "outputs": [],
      "source": [
        "# Training helper functions\n",
        "\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "\n",
        "def fit(epochs, model, train_loader, val_loader, criterion, optimizer, scheduler, device, model_name, model_save_folder, patch=False):\n",
        "    torch.cuda.empty_cache()\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    val_iou = []\n",
        "    val_acc = []\n",
        "    train_iou = []\n",
        "    train_acc = []\n",
        "    lrs = []\n",
        "    min_loss = np.inf\n",
        "    decrease = 1\n",
        "    not_improve = 0\n",
        "\n",
        "    model.to(device)\n",
        "    fit_time = time.time()\n",
        "    for e in range(epochs):\n",
        "        since = time.time()\n",
        "        running_loss = 0\n",
        "        iou_score = 0\n",
        "        accuracy = 0\n",
        "        # training loop\n",
        "        model.train()\n",
        "        for i, data in enumerate(tqdm(train_loader)):\n",
        "            # training phase\n",
        "            image_tiles, mask_tiles = data\n",
        "            if patch:\n",
        "                bs, n_tiles, c, h, w = image_tiles.size()\n",
        "\n",
        "                image_tiles = image_tiles.view(-1, c, h, w)\n",
        "                mask_tiles = mask_tiles.view(-1, h, w)\n",
        "\n",
        "            image = image_tiles.to(device);\n",
        "            mask = mask_tiles.to(device);\n",
        "            # forward\n",
        "            output = model(image)\n",
        "            loss = criterion(output, mask)\n",
        "            # evaluation metrics\n",
        "            iou_score += mIoU(output, mask)\n",
        "            accuracy += pixel_accuracy(output, mask)\n",
        "            # backward\n",
        "            loss.backward()\n",
        "            optimizer.step()  # update weight\n",
        "            optimizer.zero_grad()  # reset gradient\n",
        "\n",
        "            # step the learning rate\n",
        "            lrs.append(get_lr(optimizer))\n",
        "            scheduler.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        else:\n",
        "            model.eval()\n",
        "            test_loss = 0\n",
        "            test_accuracy = 0\n",
        "            val_iou_score = 0\n",
        "            # validation loop\n",
        "            with torch.no_grad():\n",
        "                for i, data in enumerate(tqdm(val_loader)):\n",
        "                    # reshape to 9 patches from single image, delete batch size\n",
        "                    image_tiles, mask_tiles = data\n",
        "\n",
        "                    if patch:\n",
        "                        bs, n_tiles, c, h, w = image_tiles.size()\n",
        "\n",
        "                        image_tiles = image_tiles.view(-1, c, h, w)\n",
        "                        mask_tiles = mask_tiles.view(-1, h, w)\n",
        "\n",
        "                    image = image_tiles.to(device);\n",
        "                    mask = mask_tiles.to(device);\n",
        "                    output = model(image)\n",
        "                    # evaluation metrics\n",
        "                    val_iou_score += mIoU(output, mask)\n",
        "                    test_accuracy += pixel_accuracy(output, mask)\n",
        "                    # loss\n",
        "                    loss = criterion(output, mask)\n",
        "                    test_loss += loss.item()\n",
        "\n",
        "            # calculatio mean for each batch\n",
        "            train_losses.append(running_loss / len(train_loader))\n",
        "            test_losses.append(test_loss / len(val_loader))\n",
        "\n",
        "            if min_loss > (test_loss / len(val_loader)):\n",
        "                print('Loss Decreasing.. {:.3f} >> {:.3f} '.format(min_loss, (test_loss / len(val_loader))))\n",
        "                min_loss = (test_loss / len(val_loader))\n",
        "                decrease += 1\n",
        "                if decrease % 5 == 0:\n",
        "                    print('saving model...')\n",
        "                    torch.save(model, f'{model_save_folder}/{model_name}_mIOU{str(round(val_iou_score/len(val_loader), 3)).split(\".\")[1]}_whole_model.pt')\n",
        "                    torch.save(model.state_dict(), f'{model_save_folder}/{model_name}_mIOU{str(round(val_iou_score/len(val_loader), 3)).split(\".\")[1]}_state_dict.pt')\n",
        "\n",
        "\n",
        "            if (test_loss / len(val_loader)) > min_loss:\n",
        "                not_improve += 1\n",
        "                min_loss = (test_loss / len(val_loader))\n",
        "                print(f'Loss Not Decrease for {not_improve} time')\n",
        "                if not_improve == 7:\n",
        "                    print('Loss not decrease for 7 times, Stop Training')\n",
        "                    break\n",
        "\n",
        "            # iou\n",
        "            val_iou.append(val_iou_score / len(val_loader))\n",
        "            train_iou.append(iou_score / len(train_loader))\n",
        "            train_acc.append(accuracy / len(train_loader))\n",
        "            val_acc.append(test_accuracy / len(val_loader))\n",
        "            print(\"Epoch:{}/{}..\".format(e + 1, epochs),\n",
        "                  \"Train Loss: {:.3f}..\".format(running_loss / len(train_loader)),\n",
        "                  \"Val Loss: {:.3f}..\".format(test_loss / len(val_loader)),\n",
        "                  \"Train mIoU:{:.3f}..\".format(iou_score / len(train_loader)),\n",
        "                  \"Val mIoU: {:.3f}..\".format(val_iou_score / len(val_loader)),\n",
        "                  \"Train Acc:{:.3f}..\".format(accuracy / len(train_loader)),\n",
        "                  \"Val Acc:{:.3f}..\".format(test_accuracy / len(val_loader)),\n",
        "                  \"Time: {:.2f}m\".format((time.time() - since) / 60))\n",
        "\n",
        "    history = {'train_loss': train_losses, 'val_loss': test_losses,\n",
        "               'train_miou': train_iou, 'val_miou': val_iou,\n",
        "               'train_acc': train_acc, 'val_acc': val_acc,\n",
        "               'lrs': lrs}\n",
        "    print('Total time: {:.2f} m'.format((time.time() - fit_time) / 60))\n",
        "    return history\n",
        "\n",
        "\n",
        "def predict_image_mask_miou(model, image, mask, device, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "    model.eval()\n",
        "    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
        "    image = t(image)\n",
        "    model.to(device)\n",
        "    image = image.to(device)\n",
        "    mask = mask.to(device)\n",
        "    with torch.no_grad():\n",
        "        image = image.unsqueeze(0)\n",
        "        mask = mask.unsqueeze(0)\n",
        "\n",
        "        output = model(image)\n",
        "        score = mIoU(output, mask)\n",
        "        masked = torch.argmax(output, dim=1)\n",
        "        masked = masked.cpu().squeeze(0)\n",
        "    return masked, score\n",
        "\n",
        "\n",
        "def predict_image_mask_pixel(model, image, mask, device, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "    model.eval()\n",
        "    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
        "    image = t(image)\n",
        "    model.to(device);\n",
        "    image = image.to(device)\n",
        "    mask = mask.to(device)\n",
        "    with torch.no_grad():\n",
        "        image = image.unsqueeze(0)\n",
        "        mask = mask.unsqueeze(0)\n",
        "\n",
        "        output = model(image)\n",
        "        acc = pixel_accuracy(output, mask)\n",
        "        masked = torch.argmax(output, dim=1)\n",
        "        masked = masked.cpu().squeeze(0)\n",
        "    return masked, acc\n",
        "\n",
        "\n",
        "def miou_score(model, test_set, device):\n",
        "    score_iou = []\n",
        "    for i in tqdm(range(len(test_set))):\n",
        "        img, mask = test_set[i]\n",
        "        pred_mask, score = predict_image_mask_miou(model, img, mask, device)\n",
        "        score_iou.append(score)\n",
        "    return score_iou\n",
        "\n",
        "\n",
        "def pixel_acc_score(model, test_set, device):\n",
        "    accuracy = []\n",
        "    for i in tqdm(range(len(test_set))):\n",
        "        img, mask = test_set[i]\n",
        "        pred_mask, acc = predict_image_mask_pixel(model, img, mask, device)\n",
        "        accuracy.append(acc)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lon-Q2Tl6Bwp"
      },
      "outputs": [],
      "source": [
        "# Utilitary functions\n",
        "\n",
        "def create_df(image_path):\n",
        "    name = []\n",
        "    for dirname, _, filenames in os.walk(image_path):\n",
        "        for filename in filenames:\n",
        "            name.append(filename.split('.')[0])\n",
        "\n",
        "    return pd.DataFrame({'id': name}, index=np.arange(0, len(name)))\n",
        "\n",
        "\n",
        "def make_folder(dp):\n",
        "    if not os.path.exists(dp):\n",
        "        os.mkdir(dp)\n",
        "    return\n",
        "\n",
        "\n",
        "def open_class_csv(filepath):\n",
        "    data = pd.read_csv(filepath)\n",
        "    data.columns = ['name', 'r', 'g', 'b']\n",
        "\n",
        "    # remove \"conflicting\" row\n",
        "    data = data[data['name'] != 'conflicting']\n",
        "\n",
        "    return data\n",
        "\n",
        "def create_image_legend(class_to_rgb, class_labels):\n",
        "    # Parameters for the layout\n",
        "    square_size = 50  # Size of each square\n",
        "    num_classes = len(class_to_rgb)\n",
        "    legend_width = 200  # Width of the legend\n",
        "    image_width = square_size + legend_width\n",
        "    image_height = num_classes * square_size\n",
        "\n",
        "    # Create a blank image\n",
        "    img = Image.new('RGB', (image_width, image_height), color='white')\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    # Add squares and corresponding labels\n",
        "    for i, (class_id, rgb) in enumerate(class_to_rgb.items()):\n",
        "        # Draw the square\n",
        "        top_left = (0, i * square_size)\n",
        "        bottom_right = (square_size, (i + 1) * square_size)\n",
        "        draw.rectangle([top_left, bottom_right], fill=rgb)\n",
        "\n",
        "        # Add text labels (class name)\n",
        "        text_position = (square_size + 10, i * square_size + 10)\n",
        "        draw.text(text_position, class_labels.loc[class_id]['name'], fill='black')\n",
        "\n",
        "    # Save and display the image\n",
        "    #img.show()\n",
        "    img.save('colored_squares_with_legend.png')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GWFzKUp7-uq",
        "outputId": "adfd3116-e143-44b0-87b6-667fa329f93a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# data access to google cloud\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tenJBgi_iVn",
        "outputId": "51399c28-f8aa-4700-9b80-43ceaca75512"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/ethiopia_project/data/uav_graz/dataset/semantic_drone_dataset /content/drive/MyDrive/ethiopia_project/output/uav_graz\n",
            "/content/drive/MyDrive/ethiopia_project/data/uav_graz/dataset/semantic_drone_dataset/original_images /content/drive/MyDrive/ethiopia_project/data/uav_graz/dataset/semantic_drone_dataset/label_images_semantic\n"
          ]
        }
      ],
      "source": [
        "#!ls\n",
        "BASE = '/content/drive/MyDrive/ethiopia_project'\n",
        "\n",
        "DATA_BASE = os.path.join(BASE, 'data/uav_graz/dataset/semantic_drone_dataset')\n",
        "OUTPUT_BASE = os.path.join(BASE, 'output/uav_graz')\n",
        "\n",
        "IMAGE_PATH = os.path.join(DATA_BASE, 'original_images')\n",
        "MASK_PATH = os.path.join(DATA_BASE, 'label_images_semantic')\n",
        "\n",
        "# DATA_BASE = r'C:\\Users\\PC\\Coding\\ethiopia_uav_segmentation\\data\\uav_addis_01'\n",
        "# OUTPUT_BASE = r'C:\\Users\\PC\\Coding\\ethiopia_uav_segmentation\\output\\uav_addis_01'\n",
        "\n",
        "# IMAGE_PATH = DATA_BASE\n",
        "# MASK_PATH = DATA_BASE\n",
        "\n",
        "# MODEL_BASE = r'C:\\Users\\PC\\Coding\\ethiopia_uav_segmentation\\models'\n",
        "\n",
        "print(DATA_BASE, OUTPUT_BASE)\n",
        "print(IMAGE_PATH, MASK_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eoczYMKIhA4",
        "outputId": "34436037-7e6d-464a-d5cd-0548d179d8dd"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Images:  400\n",
            "Train Size   :  306\n",
            "Val Size     :  54\n",
            "Test Size    :  40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 71/77 [59:42<04:55, 49.27s/it]"
          ]
        }
      ],
      "source": [
        "# https://www.kaggle.com/code/ligtfeather/semantic-segmentation-is-easy-with-pytorch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f'Training on device: {device}')\n",
        "\n",
        "show_example_image = False\n",
        "\n",
        "# create datasets from the provided imagery\n",
        "n_classes = 23\n",
        "df = create_df(image_path=IMAGE_PATH)\n",
        "print('Total Images: ', len(df))\n",
        "\n",
        "# create test and train datasets: training 76.5?%), testing (13.5%), validation (10%)\n",
        "X_trainval, X_test = train_test_split(df['id'].values, test_size=0.1, random_state=19)\n",
        "X_train, X_val = train_test_split(X_trainval, test_size=0.15, random_state=19)\n",
        "\n",
        "print('Train Size   : ', len(X_train))\n",
        "print('Val Size     : ', len(X_val))\n",
        "print('Test Size    : ', len(X_test))\n",
        "\n",
        "if show_example_image:\n",
        "    img = Image.open(IMAGE_PATH + '/' + df['id'][100] + '.jpg')\n",
        "    mask = Image.open(MASK_PATH + '/' + df['id'][100] + '.png')\n",
        "    print('Image Size', np.asarray(img).shape)\n",
        "    print('Mask Size', np.asarray(mask).shape)\n",
        "\n",
        "    plt.imshow(img)\n",
        "    plt.imshow(mask, alpha=0.6)\n",
        "    plt.title('Picture with Mask Appplied')\n",
        "    plt.show()\n",
        "\n",
        "# what does this do?\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "# create a  datset for re-configuration of Droone Dataset?\n",
        "t_train = A.Compose([A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST), A.HorizontalFlip(), A.VerticalFlip(),\n",
        "                      A.GridDistortion(p=0.2), A.RandomBrightnessContrast((0, 0.5), (0, 0.5)),\n",
        "                      A.GaussNoise()])\n",
        "\n",
        "t_val = A.Compose([A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST), A.HorizontalFlip(),\n",
        "                    A.GridDistortion(p=0.2)])\n",
        "\n",
        "# apply drone dataset configurations\n",
        "train_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_train, mean, std, t_train, patch=False)\n",
        "val_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_val, mean, std, t_val, patch=False)\n",
        "\n",
        "# dataloader\n",
        "batch_size = 4\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model_name = 'Unet-Mobilenet_v2_29102024'\n",
        "\n",
        "# define model and hyperparameters\n",
        "if 'Mobilenet' in model_name:\n",
        "    model = smp.Unet('mobilenet_v2', encoder_weights='imagenet', classes=23, activation=None,\n",
        "                      encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16])\n",
        "elif 'Resnet34' in model_name:\n",
        "    model = smp.Unet('resnet34', encoder_weights='imagenet', classes=23, activation=None,\n",
        "                      encoder_depth=5)\n",
        "elif 'Resnext50' in model_name:\n",
        "    model = smp.Unet('resnext50_32x4d', encoder_weights='imagenet', classes=23, activation=None,\n",
        "                      encoder_depth=5)\n",
        "\n",
        "max_lr = 1e-3\n",
        "epoch = 50  #1  #6 #15\n",
        "weight_decay = 1e-4\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
        "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epoch,\n",
        "                                            steps_per_epoch=len(train_loader))\n",
        "\n",
        "model_save_folder = os.path.join(BASE, 'models/mobilenet_showcase')\n",
        "make_folder(model_save_folder)\n",
        "\n",
        "history = fit(epoch, model, train_loader, val_loader, criterion, optimizer, sched, device,\n",
        "              model_name=model_name, model_save_folder=model_save_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hxxX2O66KUI"
      },
      "outputs": [],
      "source": [
        "# Inference of the model\n",
        "\n",
        "# https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-across-devices\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "models_folder = os.listdir(MODEL_BASE)\n",
        "print(f'available models: {models_folder}')\n",
        "\n",
        "mode = 'whole_model' #'state_dict'  # 'whole_model'\n",
        "\n",
        "#model_name = f'Unet-Mobilenet_v2_161024_mIoU385_{mode}.pt'\n",
        "model_name = f'Unet-resnext50_32x4d_211024_mIOU572_{mode}.pt'\n",
        "model_folder = 'resnext50'\n",
        "\n",
        "# model_name = f'Unet-Resnet34_181024_mIoU454_{mode}.pt'\n",
        "\n",
        "#model_name = f'Unet-resnext50_32x4d_211024_mIOU572_{mode}.pt'\n",
        "#model_folder = 'resnext50'\n",
        "MODEL_DICT_PATH = os.path.join(MODEL_BASE, model_folder, model_name)\n",
        "\n",
        "# create saving directory\n",
        "output_folder = os.path.join(OUTPUT_BASE, model_name.split('.')[0])\n",
        "make_folder(output_folder)\n",
        "make_folder(os.path.join(output_folder, 'predictions'))\n",
        "make_folder(os.path.join(output_folder, 'predictions_plots'))\n",
        "\n",
        "class_dict = open_class_csv(r'C:\\Users\\PC\\Coding\\ethiopia_uav_segmentation\\data\\uav_graz\\class_dict_seg.csv')\n",
        "class_colors = {i: (row['r'], row['g'], row['b']) for i, row in class_dict.iterrows()}\n",
        "\n",
        "#create_image_legend(class_colors, class_dict)\n",
        "\n",
        "load_state_dict = False\n",
        "load_whole_model = not load_state_dict\n",
        "\n",
        "plot_image = True\n",
        "save_predictions = True\n",
        "save_plots = True\n",
        "\n",
        "print('Creating a DataSet')\n",
        "n_classes = 23\n",
        "image_hw = (704, 1056)\n",
        "df = create_df(image_path=IMAGE_PATH)\n",
        "print('Total Images: ', len(df))\n",
        "\n",
        "# create test and train datasets: training 76.5?%), testing (13.5%), validation (10%)\n",
        "test_size = 0.1\n",
        "predict_all = True\n",
        "if predict_all:\n",
        "    test_size = 0.9\n",
        "\n",
        "X_trainval, X_test = train_test_split(df['id'].values, test_size=test_size, random_state=19)\n",
        "\n",
        "# load model\n",
        "print('Loading model')\n",
        "if 'Mobilenet' in model_name:\n",
        "    model = smp.Unet('mobilenet_v2', encoder_weights='imagenet', classes=23, activation=None,\n",
        "                      encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16])\n",
        "elif 'Resnet34' in model_name:\n",
        "    model = smp.Unet('resnet34', encoder_weights='imagenet', classes=23, activation=None,\n",
        "                      encoder_depth=5)\n",
        "elif 'Resnext50' in model_name:\n",
        "    model = smp.Unet('resnext50_32x4d', encoder_weights='imagenet', classes=23, activation=None,\n",
        "                      encoder_depth=5)\n",
        "\n",
        "print('Loading weights')\n",
        "# load model state_dict\n",
        "if load_state_dict:\n",
        "    model.load_state_dict(torch.load(MODEL_DICT_PATH, weights_only=True))\n",
        "    model.eval()\n",
        "\n",
        "if load_whole_model:\n",
        "    model = torch.load(MODEL_DICT_PATH, weights_only=False, map_location=device)\n",
        "    model.eval()\n",
        "\n",
        "# create test dataset\n",
        "t_test = A.Resize(768, 1152, interpolation=cv2.INTER_NEAREST)\n",
        "test_set = DroneTestDataset(IMAGE_PATH, MASK_PATH, X_test, transform=t_test, mask_post='.jpg')\n",
        "#prediction_set = DroneTestDataset(IMAGE_PATH, MASK_PATH, X_trainval, transform=t_test)\n",
        "pred_image_nr = None #[236] # None\n",
        "if pred_image_nr is not None:\n",
        "    test_set = DroneTestDataset(IMAGE_PATH, MASK_PATH, X_trainval, transform=t_test)\n",
        "\n",
        "\n",
        "all_gt, all_pred = [], []\n",
        "\n",
        "colored_image_np = np.zeros((768, 1152, 3), dtype=np.uint8)\n",
        "\n",
        "print('Doing inference...')\n",
        "for i, (image, mask) in tqdm(enumerate(test_set)):\n",
        "    if pred_image_nr is not None:\n",
        "        if i not in pred_image_nr:\n",
        "            continue\n",
        "    pred_mask, score = predict_image_mask_miou(model, image, mask, device)\n",
        "    img_index = test_set.X[i]\n",
        "\n",
        "    # color prediction mask\n",
        "    pred_mask_np = pred_mask.numpy()\n",
        "\n",
        "    for class_value, rgb in class_colors.items():\n",
        "        colored_image_np[pred_mask_np == class_value] = rgb\n",
        "\n",
        "    colored_image = Image.fromarray(colored_image_np)\n",
        "\n",
        "    # calculate confusion values\n",
        "    all_gt.append(mask.view(-1).cpu().numpy())\n",
        "    all_pred.append(pred_mask.view(-1).cpu().numpy())\n",
        "\n",
        "    color_ = True\n",
        "\n",
        "    if save_predictions:\n",
        "        if color_:\n",
        "            colored_image.save(os.path.join(output_folder, 'predictions', f'{img_index}.png'))\n",
        "        else:\n",
        "            pred_mask_img = np.array(pred_mask, dtype=np.uint8)\n",
        "            xx = Image.fromarray(pred_mask_img)\n",
        "            xx.save(os.path.join(output_folder, 'predictions', f'{img_index}.png'))\n",
        "\n",
        "    if plot_image:\n",
        "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 10))\n",
        "        ax1.imshow(image)\n",
        "        ax1.set_title('Picture')\n",
        "\n",
        "        ax2.imshow(mask)\n",
        "        ax2.set_title('Ground truth')\n",
        "        ax2.set_axis_off()\n",
        "\n",
        "        ax3.imshow(pred_mask)\n",
        "        ax3.set_title('UNet-MobileNet | mIoU {:.3f}'.format(score))\n",
        "        ax3.set_axis_off()\n",
        "        if save_plots:\n",
        "            fig.savefig(os.path.join(output_folder, 'predictions_plots', f'{img_index}.png'))\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "        plt.close(fig=fig)\n",
        "\n",
        "gt_cm = np.concatenate(all_gt)\n",
        "pred_cm = np.concatenate(all_pred)\n",
        "cm = confusion_matrix(gt_cm, pred_cm)\n",
        "cm_percentage = cm.astype('float') / cm.sum() * 100\n",
        "\n",
        "annot = np.empty_like(cm_percentage, dtype=object)\n",
        "for i in range(cm_percentage.shape[0]):\n",
        "    for j in range(cm_percentage.shape[1]):\n",
        "        if cm_percentage[i, j] == 0:\n",
        "            annot[i, j] = '0'  # Display '0' for zero values\n",
        "        else:\n",
        "            annot[i, j] = f'{cm_percentage[i, j]:.1f}'  # Display two decimals for non-zero values\n",
        "\n",
        "# classes: tree, gras, other vegetation, dirt, gravel, rocks, water, paved area, pool, person, dog, car, bicycle,\n",
        "#          roof, wall, fence, fence-pole, window, door, obstacle\n",
        "figcm, axcm = plt.subplots(1, 1, figsize=(10, 10))\n",
        "sns.heatmap(cm_percentage, annot=annot, fmt='', cbar=True, cmap='Blues', xticklabels=class_dict['name'],\n",
        "            yticklabels=class_dict['name'])\n",
        "plt.savefig('cm.png')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}